{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e48360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from calendar import monthrange\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#today = date.today()\n",
    "SAVE_FOLDER = r'C:\\tmp'\n",
    "BASE_URI = 'https://bvger.weblaw.ch/'\n",
    "\n",
    "DATE_suffix1 = 'dashboard?guiLanguage=de&filters=%5B%5B%22rulingDate%22%2C%5B%7B%22from%22%3A%22'\n",
    "DATE_suffix2 = 'T00%3A00%3A00.000Z%22%2C%22to%22%3A%22'\n",
    "DATE_suffix3 = 'T22%3A59%3A59.999Z%22%7D%5D%5D%5D&sort-field=relevance&sort-direction=relevance'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "121b8f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdays(year):\n",
    "    date_list = []\n",
    "    for i in range(12):\n",
    "        m_fillerzero = ''\n",
    "        if i <= 8:\n",
    "            m_fillerzero = '0'\n",
    "        for j in range(monthrange(year, (i+1))[1]):\n",
    "            fillerzero = ''\n",
    "            if j <= 8:\n",
    "                fillerzero = '0'\n",
    "            date_list.append(str(year)+'-'+m_fillerzero+str(i+1)+'-'+fillerzero+str(j+1))\n",
    "    return date_list\n",
    "\n",
    "def getfilters(start, end):\n",
    "    date_filters = []\n",
    "    for i in range(start,end):\n",
    "        date_filters += getdays(i)\n",
    "    return date_filters\n",
    "    \n",
    "myfilters = getfilters(2007,2024)\n",
    "\n",
    "filtered_urls = []\n",
    "for i in range(len(myfilters)):\n",
    "    if (i+2) > len(myfilters):\n",
    "        break\n",
    "    filtered_urls.append(BASE_URI+DATE_suffix1+myfilters[i]+DATE_suffix2+myfilters[i+1]+DATE_suffix3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48ebfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_urls = []\n",
    "\n",
    "def scrap_link(html_source):\n",
    "    url_list = []\n",
    "    soup = BeautifulSoup(html_source, 'html.parser').body\n",
    "    anchor_elements = soup.select('a[href]')\n",
    "    if len(anchor_elements) >= 1:\n",
    "        for element in anchor_elements:\n",
    "            url = str(element).split(' ')[1].split('\"')[1]\n",
    "            if url[1] == 'c':\n",
    "                url_list.append((BASE_URI[:-1]+url))\n",
    "\n",
    "        return url_list\n",
    "    del soup, anchor_elements, url_list\n",
    "    \n",
    "for fu in filtered_urls:\n",
    "    options = webdriver.EdgeOptions()\n",
    "    driver = webdriver.Edge(options=options)\n",
    "    #driver.manage().window().minimize()\n",
    "    driver.get(fu)\n",
    "    scroll_pause_time = 1  # Pause between each scroll\n",
    "    screen_height = driver.execute_script(\"return window.screen.height;\")  # Browser window height\n",
    "    i = 1\n",
    "    while True:\n",
    "        # Scroll down\n",
    "        driver.execute_script(f\"window.scrollTo(0, {screen_height * i});\")\n",
    "        i += 1\n",
    "        time.sleep(scroll_pause_time)\n",
    "\n",
    "        # Check if reaching the end of the page\n",
    "        scroll_height = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "        if screen_height * i > scroll_height:\n",
    "            break\n",
    "\n",
    "    html = driver.page_source\n",
    "    driver.quit()\n",
    "    new_urls = scrap_link(html)\n",
    "    if new_urls != None:\n",
    "         case_urls += new_urls\n",
    "    #del driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f700c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "abschnitt_namen = ['Sachverhalt','Erwaegunen','Urteil']\n",
    "sv = {\n",
    "  'de': 'Sachverhalt:',\n",
    "  'fr': 'Faits:',\n",
    "  'it': 'Ritenuto in fatto:'\n",
    "}\n",
    "ew = {\n",
    "  'de': 'Erwägung:',\n",
    "  'fr': 'Droit:',\n",
    "  'it': 'Considerando in diritto:'\n",
    "}\n",
    "vd = {\n",
    "  'de': 'erkennt das Bundesverwaltungsgericht:',\n",
    "  'fr': 'fédéral prononce:',\n",
    "  'it': 'federale pronuncia:'        \n",
    "}\n",
    "crit = [sv,ew,vd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "37ab5009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def part1_dissection(datablock):\n",
    "    alf_rev = string.ascii_uppercase[::-1]\n",
    "    letters = []\n",
    "    for letter in alf_rev:\n",
    "        if (datablock.find((letter+'.')) != -1):\n",
    "            letters.append(letter)\n",
    "    letters = letters[::-1]\n",
    "    \n",
    "    actual_letters = []\n",
    "    for i in range(len(letters)):\n",
    "        if letters[i] == string.ascii_uppercase[i]:\n",
    "            actual_letters.append(letters[i])\n",
    "\n",
    "    prv_idx = -1\n",
    "    cur_idx = 0\n",
    "    paragraphs_part1 = []\n",
    "    for l in actual_letters[::-1]:\n",
    "        if datablock.find((l+'. ')) != -1:\n",
    "            cur_idx = datablock.index((l+'. '))\n",
    "            paragraphs_part1.append(datablock[cur_idx:prv_idx])\n",
    "            prv_idx = cur_idx\n",
    "        else:\n",
    "            for abc in string.ascii_lowercase[::-1]:\n",
    "                if datablock.find(l+'.'+abc) != -1:\n",
    "                    cur_idx = datablock.index(l+'.'+abc)\n",
    "                    paragraphs_part1.append(datablock[cur_idx:prv_idx])\n",
    "                    prv_idx = cur_idx\n",
    "            paragraphs_part1.append(l+'.')\n",
    "\n",
    "    return paragraphs_part1[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82de88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def part2_dissection(datablock):\n",
    "    i = 1\n",
    "    found_is = []\n",
    "    while (datablock.find(str(i)+'. ') != -1):\n",
    "        found_is.append(i)\n",
    "        i += 1\n",
    "\n",
    "    index_list = []\n",
    "    for par_n in found_is:\n",
    "        cur_idx = datablock.index(str(par_n)+'. ')\n",
    "        if len(index_list) >= 1:\n",
    "            cur_idx = datablock[index_list[-1]:-1].index(str(par_n)+'. ') + index_list[-1]\n",
    "            if datablock[cur_idx-1] == '.':\n",
    "                cur_idx = datablock[cur_idx+1:-1].index(str(par_n)+'. ') + (cur_idx+1)\n",
    "        index_list.append(cur_idx)\n",
    "    index_list.append(-1)\n",
    "\n",
    "    paragraphs_part2 = []\n",
    "    for i in range(len(index_list)):\n",
    "        if i+1 < len(index_list):\n",
    "            paragraphs_part2.append(datablock[index_list[i]:index_list[i+1]])\n",
    "\n",
    "    sub_is =[]\n",
    "    sub_is_main = []\n",
    "    for par in part2_paragraphs:\n",
    "        i = 1\n",
    "        dot_idx = par.find('.')+1\n",
    "        pre_i = par[0:dot_idx]\n",
    "        while (par.find(pre_i+str(i)) != -1):\n",
    "            if pre_i[:-1] not in sub_is:\n",
    "                sub_is.append(pre_i)#[:-1])\n",
    "            sub_is.append(pre_i+str(i))\n",
    "            if int(pre_i[:-1])-1 not in sub_is_main:\n",
    "                sub_is_main.append(int(pre_i[:-1])-1)\n",
    "            i += 1\n",
    "    print(sub_is)\n",
    "    print(sub_is_main)\n",
    "\n",
    "    sub_pars =[]\n",
    "    prev_par_n = -1\n",
    "    for i in range(len(sub_is_main)-1,-1,-1):\n",
    "        sub_par_parts = []\n",
    "        pre_idx = -1\n",
    "        for j in range(len(sub_is)-1,-1,-1):\n",
    "            pre_i = sub_is[j][0:sub_is[j].find('.')]\n",
    "            if paragraphs_part2[sub_is_main[i]].find(sub_is[j]+'. ') != -1 and int(pre_i) == (sub_is_main[i]+1):\n",
    "                idx = paragraphs_part2[sub_is_main[i]].find(sub_is[j]+'. ')\n",
    "                sub_par_parts.append(paragraphs_part2[sub_is_main[i]][idx:pre_idx])\n",
    "                pre_idx = idx\n",
    "        sub_pars.append(sub_par_parts[::-1])\n",
    "    sub_pars = sub_pars[::-1]\n",
    "\n",
    "    for i in range(len(sub_is_main)):\n",
    "        paragraphs_part2[sub_is_main[i]] = sub_pars[i]\n",
    "    return paragraphs_part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca5005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def part3_dissection(datablock):\n",
    "    i = 1\n",
    "    found_is = []\n",
    "    while (datablock.find(str(i)+'. ') != -1):\n",
    "        found_is.append(i)\n",
    "        i += 1\n",
    "\n",
    "    index_list = []\n",
    "    for par_n in found_is:\n",
    "        cur_idx = datablock.index(str(par_n)+'. ')\n",
    "        if len(index_list) >= 1:\n",
    "            cur_idx = datablock[index_list[-1]:-1].index(str(par_n)+'. ') + index_list[-1]\n",
    "        index_list.append(cur_idx)\n",
    "    index_list.append(-1)\n",
    "\n",
    "    part3_paragraphs = []\n",
    "    for i in range(len(index_list)):\n",
    "        if i+1 < len(index_list):\n",
    "            part3_paragraphs.append(datablock[index_list[i]:index_list[i+1]])\n",
    "\n",
    "    return part3_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310ba313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_data_prep(data):\n",
    "    #json_data = {}\n",
    "    j_data ={}\n",
    "    for d in data:\n",
    "        if type(d) == list:\n",
    "            sub_json ={}\n",
    "            obj = ''\n",
    "            for sub_d in d:\n",
    "                #if type(li_li) == list:                \n",
    "                #    for li_li_li in li_li:\n",
    "                #        par2_json[li_li[0:li_li.find('. ')+1]] = li_li[li_li.find('. ')+2:-1]\n",
    "                #else:\n",
    "                sub_json[sub_d[0:sub_d.find('. ')+1]] = sub_d[sub_d.find('. ')+2:-1]\n",
    "                if obj == '':\n",
    "                    obj = sub_d[0:sub_d.find('. ')-1]\n",
    "            j_data[obj] = [sub_json]\n",
    "        else:\n",
    "            j_data[d[0:d.find('. ')+1]] = d[d.find('. ')+2:-1]\n",
    "    #json_data[partname] = [j_data]\n",
    "    return j_data #json_data\n",
    "\n",
    "def json_maker (data,doctitle,savefolder):      \n",
    "    json_fn = savefolder+'\\\\'+doctitle+'.json'\n",
    "    with open(json_fn, 'w') as f:\n",
    "        json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76d99534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIT \\nRitenuto in fatto:\\nConsiderando in diritto:\\nPer questi motivi, il Tribunale amministrativo federale pronuncia:\\n\\nFR\\nFaits:\\nDroit:\\nPar ces motifs, le Tribunal administratif fédéral prononce :\\n\\nDE\\nSachverhalt:\\nDas Bundesverwaltungsgericht zieht in Erwägung:\\nDemnach erkennt das Bundesverwaltungsgericht:\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def la_cuisine(bouillon,lang,doctitle):\n",
    "    criteria1 = crit[0][lang]\n",
    "    criteria2 = crit[1][lang]\n",
    "    criteria3 = crit[2][lang]\n",
    "\n",
    "    str_thepart  = str(bouillon)\n",
    "    cr3_idx = str_thepart.index(criteria3)\n",
    "    cr2_idx = str_thepart.index(criteria2)\n",
    "    cr1_idx = str_thepart.index(criteria1)\n",
    "    part3 = (str_thepart[cr3_idx:]).replace('</div>','').replace('</p>','').replace('<p>',' ').replace('  ',' ')\n",
    "    part2_idx = str_thepart[cr2_idx:cr3_idx][::-1].index('.')-1\n",
    "    part2 = (str_thepart[cr2_idx:cr3_idx-part2_idx]).replace('</p>','').replace('<p>',' ').replace('  ',' ')\n",
    "    part1_idx = str_thepart[cr1_idx:cr2_idx][::-1].index('.')-1\n",
    "    part1 = (str_thepart[cr1_idx:cr2_idx-part1_idx]).replace('</p>','').replace('<p>',' ').replace('  ',' ')\n",
    "    part3 = ' '.join(part3.split())\n",
    "    part2 = ' '.join(part2.split())\n",
    "    part1 = ' '.join(part1.split())\n",
    "    \n",
    "    part1_new = part1_dissection(part1)\n",
    "    part2_new = part2_dissection(part2)\n",
    "    part3_new = part3_dissection(part3)\n",
    "    \n",
    "    final_data = jason_data_prep(part1_new,part2_new,part3_new)\n",
    "    json_data['name'] = doctitle\n",
    "    json_data[abschnitt_namen[0]]= json_data_prep(part1_new,abschnitt_namen[0])\n",
    "    json_data[abschnitt_namen[1]]= json_data_prep(part2_new,abschnitt_namen[1])\n",
    "    json_data[abschnitt_namen[2]]= json_data_prep(part3_new,abschnitt_namen[2])\n",
    "    \n",
    "    jason_maker(json_data,doctitle,SAVE_FOLDER)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea99643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for case_url in case_urls:\n",
    "    driver = webdriver.Edge()\n",
    "    driver.get(case_url)\n",
    "    time.sleep(1)\n",
    "    html = driver.page_source\n",
    "    driver.quit()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    ent_name = soup.title.string\n",
    "    thepart = (soup.find(\"div\",{\"id\":\"customContentSegment\"})).find_all('div')[-2]\n",
    "    lng = soup.find('a',{\"class\":\"navig loose selected_page\"}).string\n",
    "    la_cuisine(thepart,lng,ent_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
